# -*- coding: utf-8 -*-
"""documentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s-Zw_qJKnN0jNmtab0LgwHil5jrVRz2r

### Hello! Welcome to my portfolio, Iâ€™m Sals.

As a final project that will complete my journey through a virtual internship at Kalbe Nutritionals, I will develop predictive data models to improve the company's business such as optimizing business competitive strategies or creating regression and clustering analysis, then preparing visual media to present solutions to clients.

I invite you to explore my portfolio and review my work. As I believe in continuous learning and growth, I am open to any thoughts or recommendations you may have.

Feel free to connect and reach me on [Linked In](https://www.linkedin.com/in/salsabila-mardhiyah/)!

### Setup
"""

import warnings
import itertools
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings("ignore")
plt.style.use('fivethirtyeight')
import pandas as pd
import statsmodels.api as sm
import matplotlib
matplotlib.rcParams['axes.labelsize'] = 14
matplotlib.rcParams['xtick.labelsize'] = 12
matplotlib.rcParams['ytick.labelsize'] = 12
matplotlib.rcParams['text.color'] = 'k'

"""### Importing Data"""

# Import transaction dataset
t_url = 'https://raw.githubusercontent.com/salsmardhiyah/kalbe-qty-prediction-cust-segmentation/main/data/transaction.csv'
df_t = pd.read_csv(t_url, delimiter=';')
df_t.sample(2)

# Import customer dataset
c_url = 'https://raw.githubusercontent.com/salsmardhiyah/kalbe-qty-prediction-cust-segmentation/main/data/customer.csv'
df_c = pd.read_csv(c_url, delimiter=';')
df_c.sample(2)

# Import product dataset
p_url = 'https://raw.githubusercontent.com/salsmardhiyah/kalbe-qty-prediction-cust-segmentation/main/data/product.csv'
df_p = pd.read_csv(p_url, delimiter=';')
df_p.sample(2)

# Import store dataset
s_url = 'https://raw.githubusercontent.com/salsmardhiyah/kalbe-qty-prediction-cust-segmentation/main/data/store.csv'
df_s = pd.read_csv(s_url, delimiter=';')
df_s.sample(2)

"""### Data Preprocessing"""

dfl = {'df_c': df_c,
      'df_p': df_p,
      'df_s': df_s,
      'df_t': df_t}

"""#### Checking Data Types"""

for df_name, df in dfl.items():
  print(f"{df_name} Information:\n")
  print(f"{df.info()}")
  print('-' * 70)

"""From information above, several columns have incorrect data types:
* `Income` column on df_c should be integer instead of object
* `Date` column on df_t should be date time instead of object
* `Latitude`,`Longitude` columns on df_s should be integer instead of object

##### Changing `Income` Column Data Type
"""

# Replacing values in Income column and change data type
df_c['Income'] = df_c['Income'].str.replace(',','.').astype(float)

# Re-checking Income values
df_c.sample()

# Re-checking Income data type
df_c.info()

"""##### Changing `Date` Column Data Type"""

# Replacing values in Date column and change data type
df_t['Date'] = pd.to_datetime(df_t['Date'])

# Re-checking Income values
df_t.sample()

# Re-checking Date data type
df_t.info()

"""##### Changing `Latitude`,`Longitude` Column Data Type"""

# Replacing values in Latitude and Longitude column and change data type
df_s['Latitude'] = df_s['Latitude'].str.replace(',','.').astype(float)
df_s['Longitude'] = df_s['Longitude'].str.replace(',','.').astype(float)

# Re-checking Latitude and Longitude values
df_s.sample()

# Re-checking Latitude and Longitude data type
df_s.info()

"""#### Handling Null Values"""

# Checking NULL values
for df_name, df in dfl.items():
  print(f"Null values in {df_name}:")
  print(f"\n{df.isnull().sum()}\n")
  print('-' * 30)

"""As be seen on information above, df_c `Marital Status` column has 3 null values. We will drop the row because it does not have a significant amount."""

# Dropping NULL values
df_c.dropna(inplace=True)

# Re-checking NULL values
df_c.isnull().sum()

"""#### Handling Duplicate Values"""

# Checking duplicated values
for df_name, df in dfl.items():
  print(f"Duplicated values in {df_name}:")
  print(f"\n{df.duplicated().sum()}\n")
  print('-' * 30)

"""There is no duplicated values on each dataframe

### Data Merging
"""

# Merging dataframes into one
df_m = df_t.merge(df_c, on='CustomerID')
df_m = df_m.merge(df_p.drop(labels='Price', axis=1), on='ProductID')
df_m = df_m.merge(df_s, on='StoreID')

# Display merged data
df_m.sample()

# Display merged data info
df_m.info()

"""### Machine Learning Regression (Time Series)

#### Creating New Data Frame
"""

# Creating new dataframe for time series regression
df_ts = df_m.groupby(['Date'], sort=True).agg({'Qty':'sum'})

# Display new dataframe on head
df_ts.head(3)

# Display new dataframe on tail
df_ts.tail(3)

"""We already have daily total quantity  sorted from January 1st to December 31st 2022 for time series forecasting.

#### Visualizing Quantity Time Series Data
"""

df_ts.plot(figsize=(20, 5))
plt.show()

from pylab import rcParams
rcParams['figure.figsize'] = 20, 10
decomposition = sm.tsa.seasonal_decompose(df_ts, model='additive')
fig = decomposition.plot()
plt.show()

"""From plots above, the seasonal trend repeats every seventh lag.

#### Split Data Train & Test
"""

# Defining data test size
test_ratio = 0.2

# Setting data limit
limit = round(df_ts.shape[0] * test_ratio)

# Divide data train and test
df_train = df_ts[:-limit]
df_test = df_ts[-limit:]

# Re-checking shape of data train and test
print(df_train.shape, df_test.shape)

df_train.tail()

"""#### Visualizing Data Train and Test"""

plt.figure(figsize=(20, 5))
sns.lineplot(data=df_train, x='Date', y='Qty')
sns.lineplot(data=df_test, x='Date', y='Qty')

plt.show()

# Decomposition plot data train
from pylab import rcParams
rcParams['figure.figsize'] = 20, 10
decomposition = sm.tsa.seasonal_decompose(df_train, model='a')
fig = decomposition.plot()
plt.show()

"""From plots above, the seasonal trend repeats every seventh lag.

#### Stationarity Check
"""

# Checking for stationarity using adfuller test
# H0 = data is not stationary
# H1 = data is stationary

from statsmodels.tsa.stattools import adfuller
from numpy import log
result = adfuller(df_train.values)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])

"""The p-value obtained from the test is less than the significance level (0.05), so the null hypothesis is rejected and we can conclude that the data is stationary.

But according to seasonal plot observation, the seasonal data is not stationary.
"""

df_traindiff = df_train['Qty'].diff(periods=7)
df_traindiff.head(10)

# Checking for stationarity using adfuller test
# H0 = data is not stationary
# H1 = data is stationary

from statsmodels.tsa.stattools import adfuller
from numpy import log
result = adfuller(df_traindiff.dropna())
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])

"""The p-value obtained from the test is less than the significance level (0.05), so the null hypothesis is rejected and we can conclude that the data is stationary.

#### Parameter Selection for ARIMA

##### By ACF & PACF Plot
"""

from statsmodels.graphics.tsaplots import plot_pacf, plot_acf

fig, ax = plt.subplots(2, 3, figsize=(15,5))

# Plotting original data
ax[0,0].plot(df_train['Qty'])
ax[0,0].set_title('Original Data')

# Plotting ACF and PACF
plot_acf(df_train.dropna(), ax=ax[0,1])
plot_pacf(df_train.dropna(), ax=ax[0,2])

# Plotting differencing data
ax[1,0].plot(df_traindiff)
ax[1,0].set_title('Differencing Data')

# Plotting ACF and PACF
plot_acf(df_traindiff.dropna(), ax=ax[1,1])
plot_pacf(df_traindiff.dropna(), ax=ax[1,2])


plt.tight_layout()

"""The seasonal arima model formed from the plots above is (0,0,0) with seasonal (0,1,1,7). However, we will also check the MA=1 and/or AR=1 combined models, so the models that might be formed are seasonal models (0,1,1,7), (1,1,0,7), or (1 ,1,1,7)."""

# Fit ARIMA(0,0,0)(0,1,1,7) seasonal model
order = (0,0,0)
seasonal_order = (0,1,1,7)
mod1 = sm.tsa.SARIMAX(df_train['Qty'], order=order, seasonal_order=seasonal_order)
fit_mod1 = mod1.fit()
print(fit_mod1.summary())

"""- Significance parameter (ma.S.L7) P value less than 0.05, therefore significant.

Residual assumptions on time series model:
- Residual white noise test (Ljung-Box) prob value more than 0.05, therefore residual is white noise.
- Residual normal distribution test  (Jarque-Bera (JB)) prob value less than 0.05,  hence not normally distributed.

Residual assumptions not meet.
"""

# Fit ARIMA(0,0,0)(1,1,1,7) seasonal model
order = (0,0,0)
seasonal_order = (1,1,1,7)
mod2 = sm.tsa.SARIMAX(df_train['Qty'], order=order, seasonal_order=seasonal_order)
fit_mod2 = mod2.fit()
print(fit_mod2.summary())

"""- Significance parameter (ma.S.L7) P value less than 0.05, therefore significant.

Residual assumptions on time series model:
- Residual white noise test (Ljung-Box) prob value more than 0.05, therefore residual is white noise.
- Residual normal distribution test  (Jarque-Bera (JB)) prob value less than 0.05,  hence not normally distributed.

Residual assumptions not meet.
"""

# Fit ARIMA(0,0,0)(1,1,0,7) seasonal model
order = (0,0,0)
seasonal_order = (1,1,0,7)
mod3 = sm.tsa.SARIMAX(df_train['Qty'], order=order, seasonal_order=seasonal_order)
fit_mod3 = mod3.fit()
print(fit_mod3.summary())

"""- Significance parameter P value less than 0.05, therefore significant.

Residual assumptions on time series model:
- Residual white noise test (Ljung-Box) prob value more than 0.05, therefore residual is white noise.
- Residual normal distribution test  (Jarque-Bera (JB)) prob value more than 0.05,  hence normally distributed.
-  Heteroskedasticity prob value more than 0.05, hence all residuals has a constant variance (homoscedastic).

Residual assumptions meet.

##### By Auto-Arima
"""

!pip install pmdarima

# Determine p and q values using auto-arima with D=1
from pmdarima.arima import auto_arima

mod_aa1 = auto_arima(df_train, start_p=0, start_q=0,
                      test='adf',
                      max_p=0, max_q=0,
                      m=7,
                      seasonal=True,
                      stationary=False,
                      start_P=0,
                      max_P=1,
                      D=1,
                      max_D=1,
                      start_Q=0,
                      max_Q=1,
                      trace=True,
                      seasonal_test='ocsb',
                      scoring='mse',
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=False)

"""Auto-Arima automatically discover the optimal order for an ARIMA model. The best model from auto-arima is the same order as model mod3 before.

#### Seasonal ARIMA Model
"""

# Predictions
mod1_pred = fit_mod1.predict(start=len(df_train), end=len(df_ts)-1)
mod2_pred = fit_mod2.predict(start=len(df_train), end=len(df_ts)-1)
mod3_pred = fit_mod3.predict(start=len(df_train), end=len(df_ts)-1)

# Create a DataFrame for plotting
df_pred = pd.DataFrame({
    'actual_train': df_train['Qty'],
    'actual_test': df_test['Qty'],
    'mod1_pred': mod1_pred,
    'mod2_pred': mod2_pred,
    'mod3_pred': mod3_pred,
})

# Plotting predictions
df_pred.plot(figsize=(20, 5))
plt.title('Time Series Predictions')
plt.ylabel('Values')

df_pred[len(df_train)+1 :].plot(figsize=(20, 5))
plt.xlabel('Time')

plt.show()

"""From the plot above, we can see roughly that the green line is closer to the actual line.

#### Model Evaluation
"""

# Defining metric evaluations function
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

def calculate_metrics(true_values, predicted_values):
    mae = mean_absolute_error(true_values, predicted_values)
    mse = mean_squared_error(true_values, predicted_values)
    mape = mean_absolute_percentage_error(true_values, predicted_values)
    return mae, mse, mape

# Calculating evaluation metrics
true_values = df_test['Qty']
models = {
  'mod1_pred': mod1_pred,
  'mod2_pred': mod2_pred,
  'mod3_pred': mod3_pred
}

for name, mod in models.items():
  metrics = calculate_metrics(true_values, mod)
  print(f"Evaluation {name}: MAE({metrics[0]}), MSE({metrics[1]}), MAPE({metrics[2]})")

"""The mod1_pred and mod2_pred both have lowest MAE and MSE, but the residual assumptions are not meet.

The mod3_pred ARIMA model with p,d,q = (0,0,0)(1,1,0,7) is the best performance compared to the other models.

#### Forcasting with Best Model
"""

# Fit ARIMA(0,0,0)(1,1,0,7) seasonal model
order = (0,0,0)
seasonal_order = (1,1,0,7)
best = sm.tsa.SARIMAX(df_test['Qty'], order=order, seasonal_order=seasonal_order)
fit_best = best.fit()
print(fit_best.summary())

"""- Significance parameter P value less than 0.05, therefore significant.

Residual assumptions on time series model:
- Residual white noise test (Ljung-Box) prob value more than 0.05, therefore residual is white noise.
- Residual normal distribution test  (Jarque-Bera (JB)) prob value more than 0.05,  hence normally distributed.
-  Heteroskedasticity prob value more than 0.05, hence all residuals has a constant variance (homoscedastic).

Residual assumptions meet.
"""

best_pred = fit_best.forecast(steps=31)

# Create a DataFrame for plotting
df_pred = pd.DataFrame({
    'actual_train': df_train['Qty'],
    'actual_test': df_test['Qty'],
    'mod3_pred': mod3_pred,
    'best_pred': best_pred
})

# Plotting predictions
df_pred.plot(figsize=(20, 5))
plt.title('Time Series Predictions')
plt.ylabel('Values')

df_pred[len(df_train)+1 :].plot(figsize=(20, 5))
plt.xlabel('Time')
plt.ylabel('Values')

plt.show()

best_pred

# Display information of predicted quantity values in January 2023
df_jan23 = pd.DataFrame(data=best_pred)
df_jan23.describe()

# Sum of predicted quantity needed in January 2023
df_jan23.agg({'predicted_mean':'sum'})

"""### Machine Learning Clustering

#### Creating New Data Frame
"""

# Creating new dataframe for clustering
df_cl = df_m.groupby(['CustomerID']).agg({'TransactionID':'count',
                                          'Qty':'sum',
                                          'TotalAmount':'sum'}).reset_index()

# Displaying cluster data frame
df_cl.sample()

# Dropping CustomerID Column
df_cl = df_cl.drop(columns=['CustomerID'])
df_cl.columns

"""#### Applying Normalization"""

# Normalizing data
from sklearn.preprocessing import MinMaxScaler
cln = MinMaxScaler().fit_transform(df_cl)
df_cln = pd.DataFrame(cln, columns=df_cl.columns)
df_cln.describe()

"""#### Determine Optimum Number of Cluster

##### Elbow Method
"""

# Create function for determining number of optimum cluster k
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

def find_optimal_k_elbow(data, max_k):
    distortions = []

    for k in range(1, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=0)
        kmeans.fit(data)
        distortions.append(kmeans.inertia_)

    plt.figure(figsize=(10, 5))
    plt.plot(range(1, max_k + 1), distortions, marker='o')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Distortion')
    plt.title('Elbow Method')
    plt.style.use('ggplot')
    plt.show()

# Determining number of optimum cluster
data = df_cln
max_k = 10
find_optimal_k_elbow(data, max_k)

"""To determine the optimal number of cluster, we have to select the value of k at the elbow, ie th epoint after which distortion/inertia starts decreasing in a linear fashion. Thus for the given data, we can conclude that the optimal number of clusters is 3.

##### Silhouette Score
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

# Generating the sample data from make_blobs
# This particular setting has one distinct cluster and 3 clusters placed close
# together.
X = cln
# For reproducibility

range_n_clusters = [2, 3, 4]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters,init='k-means++',max_iter=300, random_state=142)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print(
        "For n_clusters =",
        n_clusters,
        "The average silhouette_score is :",
        silhouette_avg,
    )

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(
            np.arange(y_lower, y_upper),
            0,
            ith_cluster_silhouette_values,
            facecolor=color,
            edgecolor=color,
            alpha=0.7,
        )

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(
        X[:, 0], X[:, 1], marker=".", s=30, lw=0, alpha=0.7, c=colors, edgecolor="k"
    )

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(
        centers[:, 0],
        centers[:, 1],
        marker="o",
        c="white",
        alpha=1,
        s=200,
        edgecolor="k",
    )

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker="$%d$" % i, alpha=1, s=50, edgecolor="k")

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(
        "Silhouette analysis for KMeans clustering on sample data with n_clusters = %d"
#         % n_clusters,
        fontsize=14,
        fontweight="bold",
    )

plt.show()

"""From silhouette plot we have to select the bigger value of the coefficient average and also consider proportional distribution of the clusters formed. Thus for the given data, we can conclude that the optimal number of clusters is 3.

#### K-Means Clustering
"""

# Model fitting
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=30)
kmeans.fit(df_cln)

# Appending cluster label to initial cluster dataframe
df_cl['cluster'] = kmeans.labels_

# Displaying initial dataframe with appended cluster
df_cl.sample()

# Visualizing cluster
sns.pairplot(data=df_cl, diag_kind='kde', hue='cluster')
plt.tight_layout()

"""Above is a pairplot of each clusterâ€™s parameters. This shows different average characteristics of 3 clusters formed by the model.

"""

# Grouping data by cluster
df_clf = df_cl.groupby(['cluster']).agg({ 'TransactionID' : 'mean',
                                       'Qty' : 'mean',
                                       'TotalAmount' : 'mean'
                                }).T
df_clf

"""#### Business Recommendation

From clusters formed above, marketing team could carry out personalized marketing strategies based on the characteristics of each customer segment as follows:

1.   Cluster 1: *Regular Customer*

  Encourage customers to spend more by promoting bundles of related products they've purchased before.
  
  Recommend products based on their past buying behavior to spark interest in new categories.

2.   Cluster 0: *Mid-Level Customer*
  
  Develop loyalty programs that reward frequent transactions, encouraging to continue purchasing regularly.

  Personalized recommendations to new products or upsell complementary items.

3. Cluster 2: *High-Level Customer*

  Create exclusive programs, offering unique benefits and access to limited-edition products.

  Offer early access to new product launches or exclusive events to strengthen their loyalty.
"""